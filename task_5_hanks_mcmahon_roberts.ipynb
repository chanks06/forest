{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5: Group Project Report \n",
    "## Predicting Tree Type in the Roosevelt National Forest \n",
    "\n",
    "### Final Project | Python for Data Science \n",
    "#### MSDS, Willamette University \n",
    "by Charles Hanks, Carter McMahon, & Cleighton Roberts \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![forest](./roosevelt2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of our project was to predict the types of trees present in different areas of the Roosevelt National Forest in Colorado. We built various models trained on cartographic variables such as shadow coverage, distance to nearby landmarks, soil type, and local topography to classify tree type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The main problem we addressed was how to identify the type of a tree based on its surroundings. If we know certain characteristics about a piece of land, such as elevation or the hillshade at noon, what sort of trees (i.e. \"cover type\") would we likely find there? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Research Question\n",
    "\n",
    "Which cartographic features are most predictive of tree type in the Roosevelt National Forest?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Random Forest\n",
    "The final random forest model that we chose only differed from the default hyperparameters in that the number of estimators was set to 250 rather than 100. We decided to do that after testing higher values for min_sample_leaf, min_sample_split, and n_estimators using GridSearchCV. The results of that cross-validated hyperparameter tuning grid showed that the default values for min_sample_leaf and min_sample_split were best in our case, and that 250 was better than 100 for n_estimators. Due to our limited access to computing power, 250 was the highest value that we tested for n_estimators, so it is possible that an even higher value could have yielded better accuracy scores.\n",
    "\n",
    "### Support Vector Machine \n",
    "Given the computational expensive of training a SVM model on a dataset of this size, we utilized the RandomSearchCV tuning grid method to optimize the model. We focused on 2 hyperparameters: 'C', the misclassification penalty, and 'max_iter', the number of times the algorithm iterates. We chose a logarithmic range of values to try for 'C', and for each value, a random 'max_iter' value between 1000 and 5000. Each combination of these hyperparameters was passed through 3-fold cross validation in our grid search. From this randomized grid search, the best value for 'C' was 14.37, and the best value for 'max_iter' was 1902. Unfortunately, we saw negligible increase in model performance with the random search best parameters. \n",
    "\n",
    "### K Nearest Neighbors \n",
    "We wanted to try a KNN model for two reasons. First, KNN is a more simple and fast 'lazy learner' algorithm compared to RF and SVM. Second, we suspected that it would perform well on geospatial data given that it is based on distance between data points. The best value for hyperparameter K was 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Random Forest \n",
    "Despite our hyperparameter tuning efforts, the resulting improvement in accuracy over default settings was minimal. The tuned model produced an accuracy score of 0.952521 compared to 0.95201 with the default hyperparameter settings.\n",
    "\n",
    "### Support Vector Machine \n",
    "Our best SVM model was the baseline LinearSVC Model trained on 70% of the dataset. The accuracy score of this model was .72. \n",
    "\n",
    "### K-Nearest Neighbors \n",
    "Our KNN model's performance was a pleasant surprise. With a k = 5, The accuracy score was .92. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering \n",
    "\n",
    "We found the clusters were not apparent when graphed onto a 2d graph, as one\n",
    "eclipsed the other, making it look like the same group. To solve this problem we graphed it in 3\n",
    "dimensions based on the principal component. The last 4 principal components didnâ€™t have as\n",
    "good of a cluster, so we will just focus on the top 3.\n",
    "\n",
    "The first one is a north facing slope that is at a low altitude. We can infer that it is north\n",
    "facing because the aspect is low, but the slope is not zero. North and east facing slopes are the\n",
    "lower values of aspect. We can then assume that they are also north, because of the hillshade\n",
    "values, which are lower than all of the other principal components. North facing slopes get less\n",
    "sun, (hillshade is the brightness of the slope) while east facing slopes get quite a lot, therefore it\n",
    "is most likely a north facing slope. Additionally, this component seems to be closer to roads and\n",
    "water than the others too. Lastly, this had a very high value for the covertype, but not the\n",
    "highest, leading me to believe this was mostly of covertype 3 and 4.\n",
    "\n",
    "The second principal component is a low to mid elevation forest with a south or a\n",
    "western facing slope. As the sun rises in the east, we can assume that these are mostly\n",
    "comprised of western facing slopes because the 9am hillshade is much lower than the other\n",
    "components, while the other hillshades are not nearly as low. The covertype was low, but not\n",
    "negative so it is likely any of them.\n",
    "\n",
    "The third principal component was the most interesting, as it was the only one that split\n",
    "up the cluster into two distinct pieces, unlike the other ones which were homogenous. This one\n",
    "was a higher elevation and could have had any slope or aspect, which is probably what split up\n",
    "the kmeans group as the other two principal components were so heavily based on aspect, and\n",
    "also therefore determined hillshade. Additionally, it was very far from water, both horizontally\n",
    "and vertically. Additionally, the covertype was the only negative one, suggesting that it was\n",
    "almost entirely comprised of covertype 1, suggesting that we could ascribe all of those features\n",
    "to a preferable habitat for that covertype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cluster1](./clusters1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clusters2](clusters2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Implications \n",
    "\n",
    "\n",
    "Based on our process of training and tuning the SVM models, we can say that this dataset is not ideal for a Support Vector Machine Model. During our exploratory data analysis, we found signficant overlap among classes. That is to say, many of the different tree types share similar characteristics - after all, they are in the same forest. SVMs do not perform well with very large dataset and when classes overlap (that data set has a lot of 'noise').  These two disadvantages explain why our SVM model did not accurately predict above a .71. \n",
    "\n",
    "KNN perform better than our expectations, given how relatively simple the algorithm is compared to RF and SVM. We suspect that KNN performed well due to much of our dataset was spatial distances, and KNN detects neighbors based on Euclidean distance in n-dimensional feature space. \n",
    "\n",
    "Random Forest is a tried and true model that produces consistently good results. In our case, we attribute the strong out-of-the-box performance to the fact that our dataset contained many one-hot encoded (1/0) columns, so the model could quickly determine cover type by eliminating certain potential tree types. For example, there were trees that were never found in certain soil types. An ensemble of decision trees excels with this sort of data. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "Elevation is the cartographic feature most predictive of tree type. We arrived at this conclusion by examining the variable importance plot of our Random Forest model, as well as the results of the KBest Selection process. Other important features include how far the tree is from a road, a body of water, or from where a fire has started. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our interpretation of our modeling is that human actions have a significant impact on the the types of trees that we find in the Roosevelt National Forest. Setting aside the importance of elevation (this is expected), we see the next two important features are distance to roadways and distance to fire points. We have reason to believe that distance to human activity is connected to the types of trees we find in the forest. This is yet another example of how our species shapes our environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Top 4 Advantages and Disadvantages of Support Vector Machine or SVM](https://dhirajkumarblog.medium.com/top-4-advantages-and-disadvantages-of-support-vector-machine-or-svm-a3c06a2b107#:~:text=SVM%20algorithm%20is%20not%20suitable,samples%2C%20the%20SVM%20will%20underperform.)\n",
    "\n",
    "[Forest Cover Type Dataset](https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset)\n",
    "\n",
    "[K-Nearest Neighbors (KNN) Algorithm in Machine Learning](https://www.enjoyalgorithms.com/blog/k-nearest-neighbours-in-ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
